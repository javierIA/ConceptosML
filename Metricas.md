---
marp: true
theme: gaia
style: |
  section {
    background-color: #ECF0F1;
    color: #2C3E50;
  }
  h1, h2, h3 {
    color: #16A085;
  }
---

# M茅tricas de Evaluaci贸n de Modelos 

Cuando entrenamos modelos de machine learning, es esencial tener maneras de evaluar su rendimiento. Para las tareas de clasificaci贸n, algunas de las m茅tricas m谩s comunes incluyen la precisi贸n (accuracy), la matriz de confusi贸n, la precisi贸n (precision), la exhaustividad (recall) y la puntuaci贸n F1.

---

# Accuracy (Precisi贸n) 

La precisi贸n es la fracci贸n de predicciones que el modelo acert贸.

`Accuracy = (TP + TN) / (TP + TN + FP + FN)`

Donde:

- TP = Verdaderos Positivos
- TN = Verdaderos Negativos
- FP = Falsos Positivos
- FN = Falsos Negativos

---

# Matriz de Confusi贸n М

La matriz de confusi贸n es una tabla que se usa para describir el rendimiento de un modelo de clasificaci贸n en un conjunto de datos para los cuales se conocen los valores verdaderos.

- **Verdaderos Positivos (TP)**: Los casos en los que el modelo predijo correctamente la clase positiva.
- **Verdaderos Negativos (TN)**: Los casos en los que el modelo predijo correctamente la clase negativa.
- **Falsos Positivos (FP)**: Los casos en los que el modelo predijo incorrectamente la clase positiva.
- **Falsos Negativos (FN)**: Los casos en los que el modelo predijo incorrectamente la clase negativa.
---
![bg 70% ](https://miro.medium.com/max/712/1*Z54JgbS4DUwWSknhDCvNTQ.png)

---

# Precision y Recall 锔

- **Precision (Precisi贸n)**: La precisi贸n es la fracci贸n de predicciones correctas entre las instancias que el modelo predijo como positivas. 
`Precision = TP / (TP + FP)`

- **Recall (Exhaustividad o sensibilidad)**: La exhaustividad es la fracci贸n de instancias positivas que el modelo predijo correctamente. 
`Recall = TP / (TP + FN)`

---

# F1 Score 

El F1 Score es una medida que combina la precisi贸n y la exhaustividad. Es la media arm贸nica de ambas medidas, y da un valor m谩s alto si ambas medidas son similares.

`F1 = 2 * (Precision * Recall) / (Precision + Recall)`

---
# Error Cuadr谩tico Medio (MSE)


Descripci贸n del Error Cuadr谩tico Medio
El Error Cuadr谩tico Medio (MSE) es una m茅trica com煤n para evaluar modelos de regresi贸n. Mide el promedio de los cuadrados de los errores, es decir, la diferencia cuadrada media entre los valores estimados y los valores reales.


---

# Ejemplo de Evaluaci贸n de un Modelo de Clasificaci贸n И

- Vamos a asumir que hemos entrenado un modelo para predecir si un email es spam (la clase positiva) o no (la clase negativa). En nuestro conjunto de prueba de 1000 correos electr贸nicos, el modelo hizo las siguientes predicciones:
---
# Ejemplo de Evaluaci贸n de un Modelo de Clasificaci贸n И
- Verdaderos Positivos (TP): 200 (El modelo correctamente identific贸 200 correos electr贸nicos como spam)
- Verdaderos Negativos (TN): 600 (El modelo correctamente identific贸 600 correos electr贸nicos como no spam)
- Falsos Positivos (FP): 50 (El modelo incorrectamente identific贸 50 correos electr贸nicos como spam)
- Falsos Negativos (FN): 150 (El modelo incorrectamente identific贸 150 correos electr贸nicos como no spam)
Accuracy (Precisi贸n) 
---
Vamos a calcular la precisi贸n de nuestro modelo usando la f贸rmula: Accuracy = (TP + TN) / (TP + TN + FP + FN)

Accuracy = (200 + 600) / (200 + 600 + 50 + 150) = 800 / 1000 = 0.8 or 80%

# Nuestro modelo tiene una precisi贸n del 80%.
---
# Matriz de Confusi贸n М
Aqu铆 est谩 la matriz de confusi贸n de nuestro modelo:

-	Predicci贸n Positiva	Predicci贸n Negativa
- Positiva Verdadera	200 (TP)	150 (FN)
- Negativa Verdadera	50 (FP)	600 (TN)
---
# Precision y Recall 锔
- Vamos a calcular la precisi贸n y la exhaustividad de nuestro modelo:

- Precision (Precisi贸n): Precision = TP / (TP + FP) = 200 / (200 + 50) = 0.8 or 80%
- Recall (Exhaustividad o sensibilidad): Recall = TP / (TP + FN) = 200 / (200 + 150) = 0.57 or 57%
---
# F1 Score 
Finalmente, calculamos el F1 Score de nuestro modelo usando la f贸rmula: F1 = 2 * (Precision * Recall) / (Precision + Recall)

F1 = 2 * (0.8 * 0.57) / (0.8 + 0.57) = 0.67 or 67%

